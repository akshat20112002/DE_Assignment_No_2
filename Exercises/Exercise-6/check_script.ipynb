{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bd6977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b02bfcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/03 11:03:06 WARN Utils: Your hostname, developer, resolves to a loopback address: 127.0.1.1; using 192.168.29.13 instead (on interface enp2s0)\n",
      "25/11/03 11:03:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/03 11:03:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.13:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Exercise6-Optimized</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7cad2ed2cd70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Exercise6-Optimized\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSV folder: csv_files\n"
     ]
    }
   ],
   "source": [
    "csv_files_path = \"csv_files\"\n",
    "# Check if the folder exists\n",
    "if not os.path.exists(csv_files_path):\n",
    "    raise FileNotFoundError(f\"Folder not found: {csv_files_path}\")\n",
    "else:\n",
    "    print(f\"Found CSV folder: {csv_files_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"csv_files/Divvy_Trips_2019_Q4.csv\",header=True,inferSchema=True)\n",
    "print(\"Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "704054"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+------+------------+---------------+--------------------+-------------+--------------------+----------+------+---------+\n",
      "| trip_id|         start_time|           end_time|bikeid|tripduration|from_station_id|   from_station_name|to_station_id|     to_station_name|  usertype|gender|birthyear|\n",
      "+--------+-------------------+-------------------+------+------------+---------------+--------------------+-------------+--------------------+----------+------+---------+\n",
      "|25223640|2019-10-01 00:01:39|2019-10-01 00:17:20|  2215|       940.0|             20|Sheffield Ave & K...|          309|Leavitt St & Armi...|Subscriber|  Male|     1987|\n",
      "|25223641|2019-10-01 00:02:16|2019-10-01 00:06:34|  6328|       258.0|             19|Throop (Loomis) S...|          241| Morgan St & Polk St|Subscriber|  Male|     1998|\n",
      "|25223642|2019-10-01 00:04:32|2019-10-01 00:18:43|  3003|       850.0|             84|Milwaukee Ave & G...|          199|Wabash Ave & Gran...|Subscriber|Female|     1991|\n",
      "|25223643|2019-10-01 00:04:32|2019-10-01 00:43:43|  3275|     2,350.0|            313|Lakeview Ave & Fu...|          290|Kedzie Ave & Palm...|Subscriber|  Male|     1990|\n",
      "|25223644|2019-10-01 00:04:34|2019-10-01 00:35:42|  5294|     1,867.0|            210|Ashland Ave & Div...|          382|Western Ave & Con...|Subscriber|  Male|     1987|\n",
      "|25223645|2019-10-01 00:04:38|2019-10-01 00:10:51|  1891|       373.0|            156|Clark St & Wellin...|          226|Racine Ave & Belm...|Subscriber|Female|     1994|\n",
      "|25223646|2019-10-01 00:04:52|2019-10-01 00:22:45|  1061|     1,072.0|             84|Milwaukee Ave & G...|          142|McClurg Ct & Erie St|Subscriber|Female|     1991|\n",
      "|25223647|2019-10-01 00:04:57|2019-10-01 00:29:16|  1274|     1,458.0|            156|Clark St & Wellin...|          463|Clark St & Berwyn...|Subscriber|  Male|     1995|\n",
      "|25223648|2019-10-01 00:05:20|2019-10-01 00:29:18|  6011|     1,437.0|            156|Clark St & Wellin...|          463|Clark St & Berwyn...|Subscriber|Female|     1993|\n",
      "|25223649|2019-10-01 00:05:20|2019-10-01 02:23:46|  2957|     8,306.0|            336|Cottage Grove Ave...|          336|Cottage Grove Ave...|  Customer|  NULL|     NULL|\n",
      "|25223650|2019-10-01 00:05:30|2019-10-01 00:37:36|  2564|     1,925.0|             77|Clinton St & Madi...|          506|Spaulding Ave & A...|Subscriber|  Male|     1977|\n",
      "|25223651|2019-10-01 00:07:25|2019-10-01 00:19:53|  3601|       748.0|            198|Green St & Madiso...|          331|Halsted St & Clyb...|Subscriber|Female|     1980|\n",
      "|25223652|2019-10-01 00:08:40|2019-10-01 00:26:00|   711|     1,039.0|             66|Clinton St & Lake St|           16|Paulina Ave & Nor...|  Customer|  Male|     1994|\n",
      "|25223653|2019-10-01 00:08:52|2019-10-01 00:29:17|  5005|     1,224.0|            240|Sheridan Rd & Irv...|          230|Lincoln Ave & Ros...|  Customer|  NULL|     NULL|\n",
      "|25223655|2019-10-01 00:10:03|2019-10-01 00:15:30|  1730|       326.0|             66|Clinton St & Lake St|           88|Racine Ave & Rand...|Subscriber|  Male|     1992|\n",
      "|25223656|2019-10-01 00:10:46|2019-10-01 00:16:04|  4595|       318.0|            373|Kedzie Ave & Chic...|          377|Kedzie Ave & Lake St|  Customer|  NULL|     NULL|\n",
      "|25223657|2019-10-01 00:12:04|2019-10-01 01:13:36|  2958|     3,692.0|             74|Kingsbury St & Er...|          402|Shields Ave & 31s...|  Customer|Female|     1962|\n",
      "|25223658|2019-10-01 00:12:47|2019-10-01 00:17:55|  1489|       308.0|            289|Wells St & Concor...|          176|   Clark St & Elm St|Subscriber|  Male|     1992|\n",
      "|25223659|2019-10-01 00:13:21|2019-10-01 00:34:12|  3602|     1,251.0|            106|State St & Pearso...|          334|Lake Shore Dr & B...|  Customer|  NULL|     NULL|\n",
      "|25223660|2019-10-01 00:15:07|2019-10-01 00:40:34|  4228|     1,527.0|            133|Kingsbury St & Ki...|          673|Lincoln Park Cons...|  Customer|Female|     1995|\n",
      "+--------+-------------------+-------------------+------+------------+---------------+--------------------+-------------+--------------------+----------+------+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.partitionBy(\"from_station_name\").mode(\"overwrite\").csv(\"partition_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "080a6db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 4\n"
     ]
    }
   ],
   "source": [
    "num_partitions = len(df.rdd.glom().collect())\n",
    "print(\"Number of partitions:\", num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af2b18b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e9b75b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of partitions: 610\n"
     ]
    }
   ],
   "source": [
    "path = \"partition_files\"\n",
    "\n",
    "fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
    "p = sc._jvm.org.apache.hadoop.fs.Path(path)\n",
    "\n",
    "# Get all subdirectories (i.e. partitions)\n",
    "statuses = fs.listStatus(p)\n",
    "\n",
    "partition_dirs = [f.getPath().getName() for f in statuses if f.isDirectory()]\n",
    "\n",
    "print(f\"Total number of partitions: {len(partition_dirs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd06235e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581fcfa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
